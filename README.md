# ğŸ§  AI Brain/Mind with Memory

A state-of-the-art AI assistant with persistent memory, **cross-platform compatible** (macOS & Windows). Features **LangChain**, **LlamaIndex**, **spaCy NLP**, **RoBERTa sentiment analysis**, **emotional context tracking**, ChromaDB, and OpenRouter.

<img width="838" height="860" alt="image" src="https://github.com/user-attachments/assets/c3c8e636-6eed-40d4-adf0-4a4d95cb574c" />




<img width="890" height="738" alt="image" src="https://github.com/user-attachments/assets/b6a731a0-48fb-4ae0-b536-7f597a8817d4" />



<img width="898" height="533" alt="image" src="https://github.com/user-attachments/assets/ed65f2db-3d85-4686-8979-365a6011a574" />


## âš¡ Quick Start (30 seconds)

```bash
# 1. Setup (first time only)
cd ai-brain-memory
python3 -m venv .venv
source .venv/bin/activate          # Windows: .venv\Scripts\activate
pip install -e .
cp .env.example .env               # Then add your OpenRouter API key to .env

# 2. Run it!
python main.py                     # Basic mode (simple & fast)
# OR
python main_enhanced.py            # Enhanced mode with LangChain (recommended!)
# OR
python main_enhanced.py --llamaindex   # Enhanced + Document Q&A
```

**That's it!** You now have an AI with memory. Type `/help` for commands, `/quit` to exit.

### ğŸ“Š Which Mode Should I Use? (Decision Tree)

```
Start Here â†’ What do you need?
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            â”‚                â”‚
   Speed &      Best Overall    Document Q&A
   Simplicity   Experience      over PDFs/Files
        â”‚            â”‚                â”‚
        â†“            â†“                â†“
   main.py    main_enhanced.py   main_enhanced.py
                                   --llamaindex
        â”‚            â”‚                â”‚
    âœ… Done!     âœ… Done!          âœ… Done!
    
    Fastest      LangChain        LangChain
    Basic AI     Enabled          + RAG
```

**TL;DR:** 
- New users? â†’ `python main_enhanced.py` (best experience!)
- Need speed? â†’ `python main.py` (basic mode)
- Have documents? â†’ `python main_enhanced.py --llamaindex` (Q&A mode)

---

---

## ğŸ‰ What's New in v2.0 (Phase 2 - Oct 2024)

**Major enhancements achieving 9.0/10 score:**

- â° **Human-Readable Times**: Memory timestamps show "28 minutes ago" instead of ISO format
- ğŸ“‹ **Topic Tracking**: New `/topics` command shows what you've discussed with sentiment analysis
- ğŸ“ **Extended History**: 10-message conversation window (up from 6) for better context
- ğŸ’­ **Smart Summarization**: Long conversations (>20 messages) automatically summarized - no more context cliff!
- ğŸ” **Hybrid Search**: Enhanced memory retrieval with entity (+0.15) and keyword (+0.05) boosting

**Score progression:** 7.5 â†’ 8.5 (Phase 1) â†’ 9.0 (Phase 2) ğŸ¯

---

## ğŸ–¥ï¸ Cross-Platform Support

This project automatically detects and optimizes for your hardware:

- **macOS (Apple Silicon)**: M1/M2/M3/M4 with MPS (Metal Performance Shaders) GPU acceleration
- **Windows/Linux**: NVIDIA GPUs with CUDA acceleration
- **Fallback**: CPU processing on any platform

The system automatically:
- âœ… Detects available GPU (MPS, CUDA, or CPU)
- âœ… Loads models on the optimal device
- âœ… Uses platform-specific optimizations
- âœ… Displays device info on startup

## âœ¨ Features

### **Core Capabilities**
- ğŸ¤– **Remote Inference**: OpenRouter API for powerful LLM responses
- ğŸ§  **Persistent Memory**: ChromaDB vector storage for long-term memory
- âš¡ **Fast Embeddings**: Local sentence transformers optimized for M4 GPU
- ï¿½ **Interactive CLI**: Beautiful command-line chat interface
- ğŸ” **Semantic Search**: Retrieves relevant memories based on context
- ğŸ“Š **Memory Statistics**: Track and manage your AI's memory
- ğŸ“‹ **Topic Tracking**: NEW! `/topics` command to see conversation topics

### **Advanced NLP Pipeline**
- ğŸ”¬ **spaCy Analysis**: Named entity recognition, POS tagging, dependency parsing
- ğŸ˜Š **11-Emotion Tracking**: RoBERTa model with 11 specific emotions (joy, optimism, gratitude, etc.)
- ğŸ·ï¸ **Smart Tagging**: Automatic keyword extraction and topic identification
- ğŸ¯ **Intent Detection**: Classify questions, statements, commands, expressions
- ğŸ“ **Metadata Enrichment**: Every conversation tagged with linguistic features
- ğŸ“ˆ **Emotional Trajectory**: Track emotional patterns over conversation history

### **Intelligent Context Management** âœ¨ NEW in Phase 2
- ğŸ’­ **Conversation Summarization**: Automatic summarization for conversations >20 messages
- ğŸ“ **Extended History**: 10-message window aligned with emotional analysis  
- â° **Human-Readable Times**: "28 minutes ago" instead of ISO timestamps
- ğŸ” **Hybrid Search**: Vector similarity + entity boosting (+0.15) + keyword boosting (+0.05)
- ğŸ¯ **Smart Context**: No more "context cliff" - smooth transition from summary to details

### **AI Frameworks**
- ï¿½ **LangChain Integration**: Advanced prompt engineering & conversation chains
- ğŸ¦™ **LlamaIndex RAG**: Sophisticated retrieval-augmented generation
- ğŸ§© **Multiple Modes**: Switch between basic, LangChain, or LlamaIndex pipelines

## ğŸ—ï¸ Architecture

### **Basic Mode**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           User (Command Line Interface)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AI Brain (cli.py)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Chat Interface                                â”‚
â”‚  â€¢ Command Handling                              â”‚
â”‚  â€¢ Session Management                            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Memory Store        â”‚     â”‚    AI Inference    â”‚
â”‚   (memory.py)         â”‚     â”‚   (inference.py)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ ChromaDB (Vector)   â”‚     â”‚ â€¢ OpenRouter API   â”‚
â”‚ â€¢ Embeddings (Local)  â”‚     â”‚ â€¢ Streaming        â”‚
â”‚ â€¢ Semantic Search     â”‚     â”‚ â€¢ Context Building â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Enhanced Mode (LangChain + LlamaIndex)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           User (Command Line Interface)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Enhanced AI Brain (enhanced_cli.py)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Advanced Chat Interface                       â”‚
â”‚  â€¢ Mode Switching                                â”‚
â”‚  â€¢ Multi-Pipeline Support                        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Memory Store    â”‚  â”‚ LangChain  â”‚  â”‚ LlamaIndexâ”‚
â”‚   (memory.py)     â”‚  â”‚   Brain    â”‚  â”‚    RAG    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ ChromaDB        â”‚  â”‚ â€¢ Prompts  â”‚  â”‚ â€¢ Query   â”‚
â”‚ â€¢ Embeddings      â”‚  â”‚ â€¢ Chains   â”‚  â”‚ â€¢ Index   â”‚
â”‚ â€¢ Search          â”‚  â”‚ â€¢ Memory   â”‚  â”‚ â€¢ Nodes   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Installation

### Prerequisites
- **Python 3.10+** (3.13 recommended)
- **macOS**: Works on Intel or Apple Silicon (M1/M2/M3/M4)
- **Windows**: Works with or without NVIDIA GPU
- **Linux**: Supported (CUDA optional)

### Platform-Specific Setup

<details>
<summary><b>ğŸ macOS Installation</b></summary>

```bash
# 1. Navigate to project
cd /path/to/ai-brain-memory

# 2. Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# 3. Install dependencies
pip install -e .

# 4. Download spaCy model
python -m spacy download en_core_web_sm

# 5. Configure environment
cp .env.example .env
# Edit .env with your preferred text editor
nano .env  # or: code .env, vim .env
```

**GPU Support**: Automatically uses Apple Silicon GPU (MPS) if available.

</details>

<details>
<summary><b>ğŸªŸ Windows Installation</b></summary>

```cmd
# 1. Navigate to project
cd C:\path\to\ai-brain-memory

# 2. Create virtual environment
python -m venv .venv
.venv\Scripts\activate

# 3. Install dependencies
pip install -e .

# 4. Download spaCy model
python -m spacy download en_core_web_sm

# 5. Configure environment
copy .env.example .env
# Edit .env with Notepad or your preferred editor
notepad .env
```

**GPU Support**: 
- Automatically uses NVIDIA CUDA if available
- Works on CPU if no GPU present

**Note**: If you have an NVIDIA GPU, install CUDA Toolkit from [nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads)

</details>

<details>
<summary><b>ğŸ§ Linux Installation</b></summary>

```bash
# 1. Navigate to project
cd /path/to/ai-brain-memory

# 2. Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# 3. Install dependencies
pip install -e .

# 4. Download spaCy model
python -m spacy download en_core_web_sm

# 5. Configure environment
cp .env.example .env
# Edit .env with your preferred editor
nano .env  # or: vim .env, gedit .env
```

**GPU Support**:
- Automatically uses NVIDIA CUDA if available
- Works on CPU if no GPU present

**NVIDIA GPU Users**: Install CUDA Toolkit:
```bash
# Ubuntu/Debian
sudo apt install nvidia-cuda-toolkit

# Fedora/RHEL
sudo dnf install cuda
```

</details>

## âš™ï¸ Configuration

### LLM Backend Options

You can use either **remote APIs** (OpenRouter) or **local models** (Ollama):

#### Option 1: OpenRouter (Remote - Default)

Edit `.env`:
```bash
LLM_BACKEND=openrouter
OPENROUTER_API_KEY=your-api-key-here
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet
```

- âœ… Access to best models (GPT-4, Claude, etc.)
- âœ… No local setup required
- âŒ Requires API key and internet
- âŒ Pay per use

#### Option 2: Ollama (Local - Private & Free)

1. **Install Ollama**: Download from [ollama.ai](https://ollama.ai)
2. **Pull a model**: `ollama pull llama3.2`
3. **Configure** `.env`:
```bash
LLM_BACKEND=ollama
OLLAMA_MODEL=llama3.2
OLLAMA_BASE_URL=http://localhost:11434
```

- âœ… 100% private and free
- âœ… Works offline
- âœ… Fast (no network latency)
- âŒ Requires local installation
- âŒ Limited to open-source models

See [OLLAMA_GUIDE.md](OLLAMA_GUIDE.md) for detailed instructions.

### NLP & Embedding Configuration

```bash
# Embedding Model (GPU-accelerated)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Sentiment Analysis (RoBERTa)
SENTIMENT_MODEL=cardiffnlp/twitter-roberta-base-sentiment-latest

# spaCy Model (NER & Linguistics)
SPACY_MODEL=en_core_web_sm

# ChromaDB Storage
CHROMA_PERSIST_DIR=./chroma_db

# Memory Settings
MAX_MEMORY_ITEMS=1000
MEMORY_RELEVANCE_THRESHOLD=0.7
```

### System Prompt Customization âœ¨ NEW

You can customize the AI's personality and behavior by editing the system prompt in three ways:

#### Option 1: Use a Template File (Recommended)

Edit one of the provided templates in the `templates/` directory or use an existing one:

```bash
# Use the default template
SYSTEM_PROMPT_FILE=templates/system_prompt.txt

# Or choose a pre-configured personality
SYSTEM_PROMPT_FILE=templates/system_prompt_professional.txt  # Business tone
SYSTEM_PROMPT_FILE=templates/system_prompt_creative.txt      # Creative assistant
SYSTEM_PROMPT_FILE=templates/system_prompt_technical.txt     # Technical support
```

**Available Templates:**
- `system_prompt.txt` - Default balanced assistant
- `system_prompt_professional.txt` - Business/professional tone
- `system_prompt_creative.txt` - Creative writing assistant
- `system_prompt_technical.txt` - Technical/coding assistant

Edit the `.txt` files directly to customize the AI's personality!

#### Option 2: Set as Environment Variable

```bash
SYSTEM_PROMPT="You are a helpful AI assistant that specializes in..."
```

#### Option 3: Edit the Default in Code

Modify `ai_brain/config.py` and change the `DEFAULT_SYSTEM_PROMPT` variable.

**Priority:** File template â†’ Environment variable â†’ Default in code

See `templates/README.md` for more details on creating custom templates.

### Test Your Configuration

```bash
python test_config.py
```

This shows your current configuration and device information.

## ğŸš€ Usage

### Understanding the Different Modes

This AI Brain has **TWO main programs** you can run, each with different capabilities:

#### 1ï¸âƒ£ **Basic Mode** (`main.py`)
- **What it does**: Simple, straightforward AI chat with memory
- **Best for**: Quick conversations, testing, everyday use
- **Features**: Memory retrieval, sentiment analysis, emotional adaptation, conversation storage
- **Command**: `python main.py`

#### 2ï¸âƒ£ **Enhanced Mode** (`main_enhanced.py`)
- **What it does**: Advanced AI with sophisticated frameworks
- **Best for**: Complex tasks, research, advanced prompting, document Q&A
- **Features**: Everything from Basic + LangChain AND/OR LlamaIndex pipelines
- **Commands**:
  - `python main_enhanced.py` (defaults to LangChain mode)
  - `python main_enhanced.py --langchain` (explicitly enable LangChain)
  - `python main_enhanced.py --llamaindex` (enable both LangChain + LlamaIndex RAG)

### Command Reference Table

| Command | LangChain | LlamaIndex | Best For |
|---------|-----------|------------|----------|
| `python main.py` | âŒ | âŒ | Quick chats, testing, everyday use |
| `python main_enhanced.py` | âœ… | âŒ | **Default enhanced mode** - Better prompts & context |
| `python main_enhanced.py --langchain` | âœ… | âŒ | Same as above (explicit) |
| `python main_enhanced.py --llamaindex` | âœ… | âœ… | Document Q&A, research, RAG |

**Important Notes:**
- `python main_enhanced.py` with **no flags** defaults to **LangChain mode** (not basic mode!)
- LangChain provides better prompt engineering and context management
- LlamaIndex adds document Q&A capabilities on top of LangChain
- All modes include emotional adaptation and memory retrieval

### Quick Start Guide

**Step 1: Activate your virtual environment first (required!)**

<details>
<summary><b>ğŸ macOS</b></summary>

```bash
cd /path/to/ai-brain-memory
source .venv/bin/activate
```

</details>

<details>
<summary><b>ğŸªŸ Windows</b></summary>

```cmd
cd C:\path\to\ai-brain-memory
.venv\Scripts\activate
```

</details>

<details>
<summary><b>ğŸ§ Linux</b></summary>

```bash
cd /path/to/ai-brain-memory
source .venv/bin/activate
```

</details>

**Step 2: Choose which mode to run**

```bash
# OPTION A: Basic Mode (simple and fast)
python main.py

# OPTION B: Enhanced Mode (default - uses LangChain)
python main_enhanced.py

# OPTION C: Enhanced Mode with LangChain (explicit)
python main_enhanced.py --langchain

# OPTION D: Enhanced Mode with LlamaIndex RAG (document Q&A)
python main_enhanced.py --llamaindex
```

**ğŸ’¡ Pro Tip:** Just running `python main_enhanced.py` gives you LangChain automatically!

### Mode Comparison

| Feature | Basic Mode | Enhanced (Default) | Enhanced + RAG |
|---------|------------|-------------------|----------------|
| **Command** | `python main.py` | `python main_enhanced.py` | `python main_enhanced.py --llamaindex` |
| **LangChain** | âŒ No | âœ… Yes | âœ… Yes |
| **LlamaIndex RAG** | âŒ No | âŒ No | âœ… Yes |
| **Memory Retrieval** | âœ… Yes | âœ… Yes | âœ… Yes |
| **Emotional Adaptation** | âœ… Yes | âœ… Yes | âœ… Yes |
| **Sentiment Analysis** | âœ… Yes | âœ… Yes | âœ… Yes |
| **Advanced Prompts** | âŒ No | âœ… Yes | âœ… Yes |
| **Recent Context (5 turns)** | âœ… Yes | âœ… Yes | âœ… Yes |
| **Document Q&A** | âŒ No | âŒ No | âœ… Yes |
| **Speed** | ğŸš€ Fastest | âš¡ Fast | ğŸ’¨ Moderate |
| **Complexity** | ğŸŸ¢ Simple | ğŸŸ¡ Moderate | ğŸ”´ Advanced |

### Which Mode Should I Use?

**Use Basic Mode (`python main.py`) when:**
- You want quick, simple conversations
- You're just testing the system
- You don't need advanced features
- You want the fastest performance

**Use Enhanced Mode (`python main_enhanced.py`) when:**
- You want sophisticated prompt engineering (LangChain)
- You need better conversation flow management
- You're working on complex tasks requiring structured prompts
- You want the AI to maintain better context
- **This is the recommended default for most users!**

**Use Enhanced + RAG Mode (`python main_enhanced.py --llamaindex`) when:**
- You're doing research or Q&A over documents
- You need to chat with PDFs, notes, or documentation
- You're building a knowledge base application
- You want document-aware responses

### Loading Documents for RAG (LlamaIndex)

The LlamaIndex mode lets you ask questions about your own documents!

**Step 1: Load your documents**
```bash
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Load a single file
python scripts/load_documents.py research_paper.pdf

# Load multiple files
python scripts/load_documents.py notes.txt report.pdf documentation.md

# Load all files in a directory
python scripts/load_documents.py documents/
```

**Supported formats:** `.txt`, `.md`, `.pdf`, `.docx`

**Step 2: Ask questions about them**
```bash
python main_enhanced.py --llamaindex
```

```
[You] âœ Summarize the main findings from the research paper
ğŸ’­ Using 3 relevant documents
[AI] âœ Based on the research paper you provided, the main findings are...
```

**Example workflow:**
```bash
# 1. Create a documents folder
mkdir my_documents
mv research.pdf my_documents/
mv notes.txt my_documents/

# 2. Load all documents
python scripts/load_documents.py my_documents/

# 3. Chat with your documents
python main_enhanced.py --llamaindex
```

The documents are stored in ChromaDB and will persist across sessions!

**For a complete step-by-step guide, see [RAG_QUICKSTART.md](RAG_QUICKSTART.md)**

### Practical Examples

**Example 1: Basic everyday chat**
```bash
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
python main.py
```
```
[You] âœ My name is Alex and I love hiking
[AI] âœ Nice to meet you, Alex! Hiking is a wonderful activity...

[You] âœ What's my name?
ğŸ’­ Using 1 relevant memories
[AI] âœ Your name is Alex! You mentioned that when you told me about your love for hiking.
```

**Example 2: Enhanced mode with better context handling**
```bash
source .venv/bin/activate
python main_enhanced.py --langchain
```
```
[You] âœ I'm working on a Python project about machine learning
[AI] âœ That sounds exciting! Machine learning with Python is a great combination...

[You] âœ Can you help me with my project?
ğŸ’­ Using 1 relevant memories
ğŸ˜Š Detected emotion: positive
[AI] âœ Of course! Based on what you told me about your Python machine learning project,
      I'd be happy to help. What specific aspect are you working on?
```

**Example 3: Research mode with LlamaIndex**
```bash
source .venv/bin/activate

# First, load some documents
python scripts/load_documents.py example_documents/about_ai_brain.md

# Then chat with your documents
python main_enhanced.py --llamaindex
```
```
[You] âœ What is this AI Brain project about?
ğŸ’­ Using 3 relevant documents
[AI] âœ Based on the loaded documents, the AI Brain/Mind with Memory project 
      is a sophisticated AI assistant that combines persistent memory using 
      ChromaDB, smart retrieval with semantic search, NLP analysis with spaCy 
      and RoBERTa, and supports multiple frameworks including basic, LangChain, 
      and LlamaIndex modes...

[You] âœ What frameworks does it support?
ğŸ’­ Using 2 relevant documents
[AI] âœ The project supports three main frameworks:
      1. Basic mode - for simple conversations
      2. LangChain - for advanced prompt engineering
      3. LlamaIndex - for document Q&A and RAG...
```

*Best for document Q&A and knowledge base queries*

### What Happens on Startup

The system will automatically:
- âœ… Detect your GPU (MPS/CUDA/CPU)
- âœ… Load models on optimal device
- âœ… Connect to configured LLM backend
- âœ… Initialize memory store

Example output:
```
ğŸ–¥ï¸  System Information:
   OS: Darwin 24.6.0
   Platform: arm64
   Python: 3.13.7

ğŸš€ Compute Device:
   Device: Apple Silicon GPU (MPS)
   Type: mps

ğŸ¤– AI Brain initialized with OpenRouter model: anthropic/claude-3.5-sonnet
```

### Available Commands

- `/help` - Show help message
- `/stats` - Show memory statistics
- `/topics` - Show conversation topics with sentiment analysis (NEW!)
- `/clear` - Clear all memories
- `/exit` or `/quit` - Exit the chat

## â“ FAQ - Common Questions

### "Which file do I run?"

Run **ONE** of these:
- `python main.py` - Basic mode (fastest, simple)
- `python main_enhanced.py` - Enhanced mode with LangChain (recommended!)
- `python main_enhanced.py --llamaindex` - Enhanced + document Q&A

**Don't run both at the same time.** Pick one based on your needs.

**New to the project?** Start with `python main_enhanced.py` for the best experience!

### "What's the difference between the modes?"

**Quick answer:**
- `python main.py` = Basic AI chat (fast and simple)
- `python main_enhanced.py` = **Better prompts + context** (LangChain - recommended default)
- `python main_enhanced.py --llamaindex` = Document Q&A + everything above

**All modes include:**
- âœ… Persistent memory (ChromaDB)
- âœ… Emotional adaptation
- âœ… Recent conversation context (last 3 turns)
- âœ… Sentiment analysis

**Enhanced mode adds:**
- âœ… Advanced prompt engineering with LangChain
- âœ… Better system prompts with detailed context
- âœ… Emotional adaptation guidance in prompts

**LlamaIndex mode adds:**
- âœ… Document Q&A (PDF, TXT, MD, DOCX)
- âœ… RAG (Retrieval-Augmented Generation)
- âœ… Chat with your documents

**ğŸ’¡ Tip:** `python main_enhanced.py` (with no flags) automatically enables LangChain!

### "Do I need to activate the virtual environment?"

**YES!** Always run this first:
```bash
source .venv/bin/activate  # Mac/Linux
.venv\Scripts\activate     # Windows
```

If you forget, you'll see errors like "ModuleNotFoundError".

### "Do I need OpenRouter or Ollama?"

You need **ONE** of them:
- **OpenRouter**: Cloud-based, requires API key, uses best models (Claude, GPT-4)
- **Ollama**: Local, free, private, but requires installation

Configure your choice in `.env` file (see [Configuration](#%EF%B8%8F-configuration) section).

### "How do I know it's working?"

You should see:
```
ğŸ§  AI Brain/Mind with Memory
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[You] âœ 
```

If you see errors instead, check [Troubleshooting](#-troubleshooting).

### "What files will be created?"

The program creates:
- `chroma_db/` - Your AI's memory storage
- `logs/conversations/` - JSON logs of your chats
- `logs/prompts/` - System prompts for debugging
- `.chat_history` - Command history

All are automatically ignored by git (private to your machine).

### "Can I delete the logs?"

Yes! Safe to delete anytime:
```bash
rm -rf logs/          # Deletes all logs
rm -rf chroma_db/     # Deletes AI memory (fresh start)
```
### "How do I add documents for RAG/Q&A?"

Use the `scripts/load_documents.py` script:

```bash
# Load files
python scripts/load_documents.py my_file.pdf another_file.txt

# Load directory
python scripts/load_documents.py documents/

# Then chat with them
python main_enhanced.py --llamaindex
```

Supported: `.txt`, `.md`, `.pdf`, `.docx`

See [Loading Documents for RAG](#loading-documents-for-rag-llamaindex) for details.

### Example Session

```
ğŸ§  AI Brain/Mind with Memory
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[You] âœ My name is Mark and I'm working on AI projects

ğŸ’­ Using 0 relevant memories

[AI] âœ Nice to meet you, Mark! AI projects are exciting. What specific 
area are you focusing on?

[You] âœ What's my name?

ğŸ’­ Using 2 relevant memories

[AI] âœ Your name is Mark! You mentioned that earlier when you told me 
you're working on AI projects.
```

## How It Works

1. **User Input**: You type a message in the CLI
2. **Memory Retrieval**: System searches ChromaDB for relevant past conversations
3. **Context Building**: Combines relevant memories + recent history + emotional context
4. **Inference**: Sends to LLM (OpenRouter or Ollama) with context
5. **Response**: Streams AI response back to you
6. **Memory Storage**: Saves both your message and AI response with NLP analysis to ChromaDB

## ğŸ› Troubleshooting

### Platform-Specific Issues

<details>
<summary><b>ğŸ macOS</b></summary>

**Issue**: `ModuleNotFoundError: No module named 'mlx'`
```bash
# Solution: Reinstall dependencies
pip install -e .
```

**Issue**: `MPS backend not available`
```bash
# Solution: Update macOS to 12.3+
# Or system will fall back to CPU automatically
```

**Issue**: Permission denied when installing
```bash
# Solution: Don't use sudo with pip in venv
# Make sure venv is activated:
source .venv/bin/activate
```

</details>

<details>
<summary><b>ğŸªŸ Windows</b></summary>

**Issue**: `'python' is not recognized`
```cmd
# Solution: Use 'py' instead of 'python'
py main.py
```

**Issue**: Virtual environment activation fails
```cmd
# Solution: Use different activation script
.venv\Scripts\activate.bat   # For cmd
.venv\Scripts\Activate.ps1   # For PowerShell

# Or if PowerShell execution policy blocks:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

**Issue**: CUDA not detected (with NVIDIA GPU)
```cmd
# Solution: Install CUDA Toolkit
# Download from: https://developer.nvidia.com/cuda-downloads
# Restart after installation
```

**Issue**: `OSError: [WinError 126]` when loading spaCy
```cmd
# Solution: Install Visual C++ Redistributable
# Download from: https://aka.ms/vs/17/release/vc_redist.x64.exe
```

</details>

<details>
<summary><b>ğŸ§ Linux</b></summary>

**Issue**: `ImportError: libcudart.so not found`
```bash
# Solution: Install CUDA Toolkit
sudo apt install nvidia-cuda-toolkit  # Ubuntu/Debian
sudo dnf install cuda                  # Fedora/RHEL
```

**Issue**: Permission denied
```bash
# Solution: Don't use sudo with pip in venv
# Make sure venv is activated:
source .venv/bin/activate
```

**Issue**: `OSError: [Errno 28] No space left on device`
```bash
# Solution: Clean up space or change ChromaDB directory
# In .env:
CHROMA_PERSIST_DIR=/path/to/larger/disk/chroma_db
```

</details>

### General Issues

**Issue**: Slow performance
- **Mac M1+**: Ensure MPS is being used (check startup messages)
- **Windows/Linux**: Install CUDA for NVIDIA GPUs
- **All**: Try a smaller embedding model or switch to Ollama for faster local inference

**Issue**: Out of memory
- Use a smaller model (e.g., `OLLAMA_MODEL=llama3.2:1b`)
- Reduce `MAX_MEMORY_ITEMS` in `.env`
- Close other applications

**Issue**: API errors with OpenRouter
- Check your API key is correct in `.env`
- Verify you have credits at [openrouter.ai](https://openrouter.ai)
- Try switching to Ollama for local inference

**Issue**: ChromaDB errors
- Delete `chroma_db` folder and restart (WARNING: deletes all memories)
- Check disk space
- Verify write permissions

## ğŸ’» Performance

### By Platform

| Platform | GPU | Embedding Speed | Memory Retrieval |
|----------|-----|----------------|------------------|
| **Mac M1/M2/M3/M4** | MPS | ~10-20ms | <5ms |
| **Windows + NVIDIA** | CUDA | ~5-15ms | <5ms |
| **Linux + NVIDIA** | CUDA | ~5-15ms | <5ms |
| **Any (CPU only)** | CPU | ~100-200ms | <5ms |

### Inference Speed

- **OpenRouter**: 20-100ms/token (network dependent)
- **Ollama (local)**: 10-50ms/token (hardware dependent)
- **Memory operations**: <5ms

## ğŸ“ Project Structure

```
ai-brain-memory/
â”œâ”€â”€ ai_brain/                   # Core package
â”‚   â”œâ”€â”€ __init__.py            # Package initialization
â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ memory.py              # ChromaDB memory store + topic tracking
â”‚   â”œâ”€â”€ inference.py           # OpenRouter API + conversation summarization
â”‚   â”œâ”€â”€ cli.py                 # Command-line interface
â”‚   â”œâ”€â”€ enhanced_cli.py        # Enhanced CLI with mode switching
â”‚   â”œâ”€â”€ langchain_brain.py     # LangChain integration
â”‚   â”œâ”€â”€ llamaindex_brain.py    # LlamaIndex RAG
â”‚   â””â”€â”€ nlp_analyzer.py        # spaCy + RoBERTa NLP pipeline
â”‚
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE_REVIEW.md # Complete technical analysis
â”‚   â”œâ”€â”€ FLOW_DIAGRAM.md        # Visual processing pipeline
â”‚   â”œâ”€â”€ IMPLEMENTATION_PLAN.md # Prioritized roadmap
â”‚   â”œâ”€â”€ REMAINING_ARCHITECTURE_ITEMS.md  # Progress tracking
â”‚   â”œâ”€â”€ PHASE_2_COMPLETION_SUMMARY.md    # Phase 2 achievements
â”‚   â”œâ”€â”€ QUERY_ENHANCEMENT.md   # Query preprocessing details
â”‚   â”œâ”€â”€ EMOTIONAL_TRAJECTORY.md # 11-emotion system
â”‚   â”œâ”€â”€ TIME_FORMATTING_IMPLEMENTATION.md
â”‚   â”œâ”€â”€ TOPIC_TRACKING_IMPLEMENTATION.md
â”‚   â”œâ”€â”€ EXTENDED_HISTORY_IMPLEMENTATION.md
â”‚   â””â”€â”€ CONVERSATION_SUMMARIZATION_IMPLEMENTATION.md
â”‚
â”œâ”€â”€ tests/                      # Test suite
â”‚   â”œâ”€â”€ test_phase1_fixes.py   # Phase 1 validation
â”‚   â”œâ”€â”€ test_emotional_tracking.py  # 11-emotion system tests
â”‚   â”œâ”€â”€ test_query_enhancement.py   # Query preprocessing tests
â”‚   â”œâ”€â”€ test_topic_feature.py       # Topic tracking tests
â”‚   â”œâ”€â”€ test_conversation_summarization.py
â”‚   â””â”€â”€ ...                    # Additional test files
â”‚
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ inspect_metadata.py    # Inspect ChromaDB metadata
â”‚   â”œâ”€â”€ load_documents.py      # Load docs for RAG
â”‚   â””â”€â”€ migrate_to_cosine.py   # Database migration
â”‚
â”œâ”€â”€ example_documents/          # Sample documents for RAG
â”œâ”€â”€ main.py                    # Entry point (basic mode)
â”œâ”€â”€ main_enhanced.py           # Entry point (enhanced mode)
â”œâ”€â”€ pyproject.toml             # Dependencies
â”œâ”€â”€ requirements.txt           # Pinned dependencies
â”œâ”€â”€ .env.example               # Configuration template
â””â”€â”€ README.md                  # This file
```

## Advanced Usage

### Quick Command Reference

```bash
# Activate environment (always first!)
source .venv/bin/activate        # Mac/Linux
.venv\Scripts\activate           # Windows

# Run modes
python main.py                        # Basic chat
python main_enhanced.py --langchain   # Advanced prompts
python main_enhanced.py --llamaindex  # Document Q&A

# Load documents for RAG
python scripts/load_documents.py file.pdf      # Single file
python scripts/load_documents.py docs/*.txt    # Multiple files
python scripts/load_documents.py documents/    # Whole directory

# Test and debug
python tests/test_config.py            # Check configuration
python tests/test_logging.py           # Test logging system
python tests/test_nlp.py              # Test NLP features

# Run all Phase 2 tests
python tests/test_topic_feature.py
python tests/test_conversation_summarization.py

# View logs
ls logs/conversations/          # Conversation JSON files
ls logs/prompts/               # System prompt logs
cat logs/prompts/prompts_*.log # View latest prompts
```

### Programmatic Access

```python
from ai_brain.memory import MemoryStore
from ai_brain.inference import AIBrain
from ai_brain.config import Config

# Initialize
memory = MemoryStore()
brain = AIBrain()

# Add a memory
memory.add_memory("User prefers Python over JavaScript", memory_type="preference")

# Retrieve memories
memories = memory.retrieve_memories("programming languages", n_results=5)

# Generate response
response = brain.generate_response(
    message="What do I prefer?",
    relevant_memories=memories,
    stream=False
)
```

## ğŸ“š Documentation

### Architecture & Design

Comprehensive documentation on system design, flow analysis, and improvements:

- **[REVIEW_SUMMARY.md](REVIEW_SUMMARY.md)** - Executive summary of architecture review (START HERE)
- **[ARCHITECTURE_REVIEW.md](ARCHITECTURE_REVIEW.md)** - Complete technical analysis of all components
- **[FLOW_DIAGRAM.md](FLOW_DIAGRAM.md)** - Visual flow diagrams showing message processing pipeline
- **[IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md)** - Prioritized roadmap for enhancements
- **[QUERY_ENHANCEMENT.md](QUERY_ENHANCEMENT.md)** - How query preprocessing works with spaCy

### Key Insights from Architecture Review

**Current Score**: 7.5/10 â†’ **Target**: 9.0/10

**What's Working Well**:
- âœ… Excellent separation of concerns (memory, NLP, inference, CLI)
- âœ… Dual-use spaCy analysis (pre-query routing + post-storage enrichment)
- âœ… Emotional intelligence with natural language summaries
- âœ… GPU-accelerated embeddings and sentiment analysis

**Priority Improvements**:
- ğŸ”´ Query enhancement deduplication (fixes "Mark Mark" duplicates)
- ğŸ”´ Hybrid search with metadata boosting (use NLP tags in retrieval)
- ğŸŸ¡ Extended emotional context window (10 messages instead of 5)
- ğŸŸ¡ Conversation summarization for long sessions
- ğŸŸ¢ Memory consolidation for long-term scalability

See `REVIEW_SUMMARY.md` for complete details and implementation timeline.

## Troubleshooting

**Issue**: `OPENROUTER_API_KEY not set`
- **Solution**: Copy `.env.example` to `.env` and add your API key

**Issue**: Slow embeddings
- **Solution**: Ensure you're using the M4 GPU (automatic with sentence-transformers)

**Issue**: ChromaDB errors
- **Solution**: Delete `./chroma_db` folder and restart

## Future Enhancements

- [ ] Add local MLX inference option
- [ ] Implement memory consolidation
- [ ] Add multi-modal memory (images, audio)
- [ ] Memory importance scoring
- [ ] Export/import conversations
- [ ] Web interface

## License

MIT License - Feel free to use and modify!

## Contributing

This is a personal project, but suggestions are welcome!
